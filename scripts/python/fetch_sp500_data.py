#!/usr/bin/env python3
"""
fetch_sp500_data.py
R√©cup√®re les donn√©es S&P500 et calcule les features pour le HMM de d√©tection de r√©gime.

Features:
1. Log Returns: log(P_t / P_{t-1})
2. Volatilit√© r√©alis√©e: std des returns sur fen√™tre glissante
3. Volume normalis√©: (V_t - mean(V)) / std(V)

Ces 3 features permettent de caract√©riser les r√©gimes de march√©:
- Bull: returns > 0, faible volatilit√©, volume mod√©r√©
- Bear: returns < 0, haute volatilit√©, volume √©lev√© (panique)
- Sideways: returns ~ 0, faible volatilit√©, faible volume
"""

import yfinance as yf
import numpy as np
import pandas as pd
import json
import struct
from pathlib import Path
from datetime import datetime
import argparse

# ============================================================================
# CONFIGURATION
# ============================================================================

# SEED GLOBAL POUR REPRODUCTIBILIT√â
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)

# Chemins
PROJECT_ROOT = Path(__file__).parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data"
PROCESSED_DIR = DATA_DIR / "processed"
RAW_DIR = DATA_DIR / "raw"


def ensure_dirs():
    """Cr√©e les r√©pertoires n√©cessaires"""
    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    RAW_DIR.mkdir(parents=True, exist_ok=True)


# ============================================================================
# ACQUISITION DES DONN√âES
# ============================================================================

def fetch_sp500_data(
    ticker: str = "SPY",
    start_date: str = "2010-01-01",
    end_date: str = "2026-01-01",
    save_raw: bool = True
) -> pd.DataFrame:
    """
    T√©l√©charge les donn√©es OHLCV du S&P500 (via SPY ETF)
    
    Args:
        ticker: Symbole (SPY pour S&P500 ETF)
        start_date: Date de d√©but
        end_date: Date de fin (None = aujourd'hui)
        save_raw: Sauvegarder les donn√©es brutes
        
    Returns:
        DataFrame avec OHLCV
    """
    print(f"üì• T√©l√©chargement {ticker} depuis {start_date}...")
    
    data = yf.download(
        ticker,
        start=start_date,
        end=end_date,
        progress=True,
        auto_adjust=True  # Ajuste pour dividendes/splits
    )
    
    if data.empty:
        raise ValueError(f"Aucune donn√©e r√©cup√©r√©e pour {ticker}")
    
    print(f"‚úì {len(data)} jours de donn√©es ({data.index[0].date()} ‚Üí {data.index[-1].date()})")
    
    if save_raw:
        raw_path = RAW_DIR / f"{ticker}_raw.csv"
        data.to_csv(raw_path)
        print(f"‚úì Donn√©es brutes sauvegard√©es: {raw_path}")
    
    return data


# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

def compute_features(
    data: pd.DataFrame,
    vol_window: int = 20,
    vol_annualize: bool = True
) -> pd.DataFrame:
    """
    Calcule les 3 features pour le HMM.
    
    Args:
        data: DataFrame OHLCV
        vol_window: Fen√™tre pour volatilit√© r√©alis√©e (jours)
        vol_annualize: Annualiser la volatilit√© (√ó‚àö252)
        
    Returns:
        DataFrame avec [returns, volatility, volume_norm]
    """
    print(f"\nüìä Calcul des features (fen√™tre vol={vol_window} jours)...")
    
    # 1. Log Returns
    close = data['Close']
    returns = np.log(close / close.shift(1))
    
    # 2. Volatilit√© r√©alis√©e (rolling std des returns)
    volatility = returns.rolling(window=vol_window).std()
    if vol_annualize:
        volatility = volatility * np.sqrt(252)
    
    # 3. Volume normalis√© (z-score)
    volume = data['Volume']
    volume_mean = volume.rolling(window=vol_window).mean()
    volume_std = volume.rolling(window=vol_window).std()
    volume_norm = (volume - volume_mean) / volume_std
    
    # Combine
    features = pd.DataFrame({
        'returns': returns.to_numpy().flatten(),
        'volatility': volatility.to_numpy().flatten(),
        'volume_norm': volume_norm.to_numpy().flatten()
    }, index=data.index)
    
    # Drop NaN (d√©but de s√©rie d√ª aux fen√™tres glissantes)
    features = features.dropna()
    
    print(f"‚úì Features calcul√©es: {len(features)} observations √ó {features.shape[1]} features")
    
    # Statistiques
    print("\nüìà Statistiques des features:")
    print(features.describe().round(6))
    
    return features


# ============================================================================
# EXPORT POUR C++
# ============================================================================

def export_for_cpp(
    features: pd.DataFrame,
    output_dir: Path = PROCESSED_DIR,
    prefix: str = "sp500"
):
    """
    Exporte les donn√©es en format binaire pour C++.
    
    Format:
    - {prefix}_observations.bin: float32 array [T √ó K], row-major
    - {prefix}_info.json: m√©tadonn√©es (T, K, dates, etc.)
    """
    T, K = features.shape
    
    # 1. Export binaire (float32, row-major)
    data_flat = features.values.astype(np.float32).flatten(order='C')
    
    bin_path = output_dir / f"{prefix}_observations.bin"
    data_flat.tofile(bin_path)
    
    print(f"\nüíæ Export binaire: {bin_path}")
    print(f"   Taille: {bin_path.stat().st_size / 1024:.1f} KB")
    print(f"   Shape: [{T} √ó {K}] = {T * K} floats")
    
    # 2. Export m√©tadonn√©es JSON
    info = {
        'T': int(T),
        'K': int(K),
        'dtype': 'float32',
        'order': 'row-major (C)',
        'features': list(features.columns),
        'date_start': str(features.index[0].date()),
        'date_end': str(features.index[-1].date()),
        'seed': GLOBAL_SEED,
        'created_at': datetime.now().isoformat()
    }
    
    json_path = output_dir / f"{prefix}_info.json"
    with open(json_path, 'w') as f:
        json.dump(info, f, indent=2)
    
    print(f"‚úì M√©tadonn√©es: {json_path}")
    
    # 3. Export CSV (pour inspection/debug)
    csv_path = output_dir / f"{prefix}_features.csv"
    features.to_csv(csv_path)
    print(f"‚úì CSV (debug): {csv_path}")
    
    # 4. Export des dimensions dans un fichier simple pour C++ facile
    dims_path = output_dir / f"{prefix}_dims.txt"
    with open(dims_path, 'w') as f:
        f.write(f"{T} {K}\n")
    print(f"‚úì Dimensions: {dims_path}")
    
    return bin_path, json_path


def export_init_params(
    T: int, K: int, N: int = 3,
    output_dir: Path = PROCESSED_DIR,
    prefix: str = "sp500"
):
    """
    G√©n√®re et exporte les param√®tres initiaux du HMM pour C++.
    
    Args:
        T, K: Dimensions des donn√©es
        N: Nombre d'√©tats cach√©s (3 = Bull/Bear/Sideways)
    """
    print(f"\nüîß G√©n√©ration des param√®tres initiaux (N={N} √©tats)...")
    
    # SEED pour reproductibilit√©
    np.random.seed(GLOBAL_SEED)
    
    # 1. Distribution initiale œÄ (uniforme)
    pi = np.full(N, 1.0 / N, dtype=np.float32)
    
    # 2. Matrice de transition A (persistante)
    # Les r√©gimes ont tendance √† persister
    persistence = 0.9
    A = np.full((N, N), (1 - persistence) / (N - 1), dtype=np.float32)
    np.fill_diagonal(A, persistence)
    
    # 3. Moyennes Œº (s√©par√©es pour les 3 r√©gimes)
    # Bull: returns positifs, vol basse, volume normal
    # Bear: returns n√©gatifs, vol haute, volume √©lev√©
    # Sideways: returns ~0, vol basse, volume bas
    mu = np.array([
        [0.001, 0.10, 0.0],    # Bull
        [-0.002, 0.30, 1.0],   # Bear
        [0.0, 0.15, -0.5],     # Sideways
    ], dtype=np.float32)
    
    # 4. Covariances Œ£ (diagonal avec petite corr√©lation)
    base_var = np.array([0.0002, 0.01, 0.5], dtype=np.float32)  # Par feature
    Sigma = np.zeros((N, K, K), dtype=np.float32)
    for i in range(N):
        # Matrice diagonale + r√©gularisation
        Sigma[i] = np.diag(base_var) * (1.0 + 0.2 * i)  # Plus de variance dans Bear
        # R√©gularisation pour garantir positive-d√©finite
        Sigma[i] += np.eye(K, dtype=np.float32) * 1e-4
    
    # Export
    paths = {}
    
    paths['pi'] = output_dir / f"{prefix}_pi_init.bin"
    pi.tofile(paths['pi'])
    
    paths['A'] = output_dir / f"{prefix}_A_init.bin"
    A.tofile(paths['A'])
    
    paths['mu'] = output_dir / f"{prefix}_mu_init.bin"
    mu.tofile(paths['mu'])
    
    paths['Sigma'] = output_dir / f"{prefix}_Sigma_init.bin"
    Sigma.tofile(paths['Sigma'])
    
    # JSON avec info
    params_info = {
        'N': int(N),
        'K': int(K),
        'seed': GLOBAL_SEED,
        'persistence': float(persistence),
        'files': {k: str(v) for k, v in paths.items()}
    }
    
    params_json_path = output_dir / f"{prefix}_params_info.json"
    with open(params_json_path, 'w') as f:
        json.dump(params_info, f, indent=2)
    
    print(f"‚úì Param√®tres export√©s:")
    for name, path in paths.items():
        print(f"   {name}: {path}")
    
    return paths


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Pr√©pare les donn√©es S&P500 pour HMM')
    parser.add_argument('--ticker', default='SPY', help='Ticker (default: SPY)')
    parser.add_argument('--start', default='2005-01-01', help='Date d√©but')
    parser.add_argument('--end', default=None, help='Date fin (default: now)')
    parser.add_argument('--vol-window', type=int, default=10, help='Fen√™tre volatilit√©')
    parser.add_argument('--n-states', type=int, default=3, help='Nombre d\'√©tats HMM')
    parser.add_argument('--prefix', default='sp500', help='Pr√©fixe fichiers output')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("  S&P500 DATA PREPARATION FOR HMM")
    print("=" * 60)
    print(f"  Seed: {GLOBAL_SEED}")
    print(f"  Ticker: {args.ticker}")
    print(f"  Period: {args.start} ‚Üí {args.end or 'now'}")
    print("=" * 60)
    
    ensure_dirs()
    
    # 1. Fetch data
    raw_data = fetch_sp500_data(
        ticker=args.ticker,
        start_date=args.start,
        end_date=args.end
    )
    
    # 2. Compute features
    features = compute_features(raw_data, vol_window=args.vol_window)
    
    # 3. Export for C++
    export_for_cpp(features, prefix=args.prefix)
    
    # 4. Generate initial params
    T, K = features.shape
    export_init_params(T, K, N=args.n_states, prefix=args.prefix)
    
    print("\n" + "=" * 60)
    print("  ‚úÖ DATA PREPARATION COMPLETE")
    print("=" * 60)
    print(f"\n  Output directory: {PROCESSED_DIR}")
    print(f"  Ready for C++ profiling!\n")


if __name__ == '__main__':
    main()